# -*- coding: utf-8 -*-
"""LLM On IPC Docipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hAZt424as8EKn5WeBKF2VrPuwFunXpzh
"""

!pip install langchain
!pip install openai
!pip install PyPDF2
!pip install faiss-cpu
!pip install tiktoken

from PyPDF2 import PdfReader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import ElasticVectorSearch, Pinecone, Weaviate, FAISS

import os
os.environ["OPENAI_API_KEY"] = "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"

# connect your Google Drive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)
root_dir = "/content/gdrive/My Drive/"

# location of the pdf file/files. 
reader = PdfReader('/content/gdrive/MyDrive/LegalAI.pdf')

reader

# read data from the file and put them into a variable called raw_text
raw_text = ''
for i, page in enumerate(reader.pages):
    text = page.extract_text()
    if text:
        raw_text += text

raw_text[:100]

# We need to split the text that we read into smaller chunks so that during information retreival we don't hit the token size limits. 

text_splitter = CharacterTextSplitter(        
    separator = "\n",
    chunk_size = 1000,
    chunk_overlap  = 200,
    length_function = len,
)
texts = text_splitter.split_text(raw_text)

len(texts)

texts[0]

texts[1]

# Download embeddings from OpenAI
embeddings = OpenAIEmbeddings()

docsearch = FAISS.from_texts(texts, embeddings)

docsearch

from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI

chain = load_qa_chain(OpenAI(), chain_type="stuff")

query = "who are the authors of the article?"
docs = docsearch.similarity_search(query)
chain.run(input_documents=docs, question=query)

query = "What is Movable property?"
docs = docsearch.similarity_search(query)
chain.run(input_documents=docs, question=query)

query = "what is Punishment for assault or criminal force otherwise than on grave provocation ?"
docs = docsearch.similarity_search(query)
chain.run(input_documents=docs, question=query)

query = "what Indian Penal Code says about Act of a child under seven years of age?"
docs = docsearch.similarity_search(query)
chain.run(input_documents=docs, question=query)

query = "According to Indian penal code if I killed someone then what will be punishment for me ?"
docs = docsearch.similarity_search(query)
chain.run(input_documents=docs, question=query)

query = "According to Indian penal code if I stole someothing then what will be punishment for me ?"
docs = docsearch.similarity_search(query)
chain.run(input_documents=docs, question=query)

query = "According to Indian penal code if   rape case by man then what will be punishment  ?"
docs = docsearch.similarity_search(query)
chain.run(input_documents=docs, question=query)

query = "According to Indian penal code what is given subsection (2)  ?"
docs = docsearch.similarity_search(query)
answer=chain.run(input_documents=docs, question=query)

answer

query = "According to this pdf, what arebest practices of an advocate ?"
docs = docsearch.similarity_search(query)
answer=chain.run(input_documents=docs, question=query)

answer

query = "According to this pdf, who comes after chief justice of supreme court of india ?"
docs = docsearch.similarity_search(query)
answer=chain.run(input_documents=docs, question=query)

answer

!pip install langchain openai PyPDF2 faiss-cpu tiktoken gradio

from PyPDF2 import PdfReader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.llms import OpenAI
import os
import gradio as gr

# set OpenAI API key
os.environ["OPENAI_API_KEY"] = "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"

# connect Google Drive
from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)
root_dir = "/content/gdrive/My Drive/"

# location of the pdf file/files
pdf_path = '/content/gdrive/MyDrive/LegalAI.pdf'

# read data from the file and put them into a variable called raw_text
with open(pdf_path, 'rb') as f:
    pdf_reader = PdfReader(f)
    raw_text = ''.join([page.extract_text() for page in pdf_reader.pages])

# split the text into smaller chunks
text_splitter = CharacterTextSplitter(
    separator="\n",
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
)
texts = text_splitter.split_text(raw_text)

# download embeddings from OpenAI
embeddings = OpenAIEmbeddings()

# create FAISS vector store from texts
docsearch = FAISS.from_texts(texts, embeddings)

# load QA chain
chain = load_qa_chain(OpenAI(), chain_type="stuff")

# define function to answer question using the loaded chain and vector store
def answer_question(query):
    docs = docsearch.similarity_search(query)
    return chain.run(input_documents=docs, question=query)


css_code='body{background-image:url("https://picsum.photos/seed/picsum/200/300");}'

# define Gradio interface
iface = gr.Interface(fn=answer_question,
                     inputs="text", 
                     outputs="text", 
                     title="LegalAI", 
                     description="Ask any question related to Indian Consitition, Judiciary, IPC and Get Answer Quickly.",
                     layout="vertical",
                     width=800,
                     theme="compact",
                     height=400,
                     css='div {margin-left: auto; margin-right: auto; width: 100%;\
            background-image: url("https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.dreamstime.com%2Fphotos-images%2Flegal.html&psig=AOvVaw2HZNufwesHD9Jpv7ujDo_F&ust=1683444303744000&source=images&cd=vfe&ved=0CBEQjRxqFwoTCMCX2--U4P4CFQAAAAAdAAAAABAH"); repeat 0 0;}'
                    
                    )

# launch the Gradio web app
iface.launch(share=True)